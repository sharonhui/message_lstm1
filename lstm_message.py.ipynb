{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuelun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.6\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "model.add(Embedding(50922+1, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM, GRU\n",
    "model.add(LSTM(128)) # try using a GRU instead, for fun\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "#neg=pd.read_excel('neg.xls',header=None,index=None)\n",
    "#pos=pd.read_excel('pos.xls',header=None,index=None) #读取训练语料完毕\n",
    "content_tag=pd.read_excel('comment_tag1.xls',header=None,index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       0\n",
       "2       1\n",
       "3       1\n",
       "4       0\n",
       "5       0\n",
       "6       0\n",
       "7       0\n",
       "8       0\n",
       "9       0\n",
       "10      0\n",
       "11      0\n",
       "12      0\n",
       "13      0\n",
       "14      0\n",
       "15      0\n",
       "16      0\n",
       "17      0\n",
       "18      0\n",
       "19      0\n",
       "20      0\n",
       "21      0\n",
       "22      0\n",
       "23      0\n",
       "24      0\n",
       "25      0\n",
       "26      0\n",
       "27      0\n",
       "28      0\n",
       "29      0\n",
       "30      0\n",
       "       ..\n",
       "2373    0\n",
       "2374    0\n",
       "2375    0\n",
       "2376    0\n",
       "2377    0\n",
       "2378    0\n",
       "2379    0\n",
       "2380    1\n",
       "2381    1\n",
       "2382    0\n",
       "2383    0\n",
       "2384    0\n",
       "2385    0\n",
       "2386    0\n",
       "2387    1\n",
       "2388    0\n",
       "2389    1\n",
       "2390    1\n",
       "2391    1\n",
       "2392    1\n",
       "2393    1\n",
       "2394    0\n",
       "2395    1\n",
       "2396    0\n",
       "2397    1\n",
       "2398    1\n",
       "2399    1\n",
       "2400    0\n",
       "2401    0\n",
       "2402    1\n",
       "Name: 1, Length: 2402, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "content_tag['mark']=content_tag[1:].iloc[:,1]\n",
    "cw = lambda x: list(jieba.cut(x))\n",
    "content_tag['words'] = content_tag[0].apply(cw)\n",
    "comment = pd.read_excel('comment.xls') #读入评论内容\n",
    "comment['words'] = comment['comment'].apply(cw) #评论分词 \n",
    "d2v_train = pd.concat([content_tag['words'], comment['words']], ignore_index = True) \n",
    "\n",
    "w = [] #将所有词语整合在一起\n",
    "for i in d2v_train:\n",
    "    w.extend(i)\n",
    "dict = pd.DataFrame(pd.Series(w).value_counts()) #统计词的出现次数\n",
    "\n",
    "del w,d2v_train\n",
    "dict['id']=list(range(1,len(dict)+1))\n",
    "\n",
    "get_sent = lambda x: list(dict['id'][x])\n",
    "content_tag['sent'] = content_tag['words'].apply(get_sent) #速度太慢，将分的词换成计算频次dataframe中的行序号，默认是按频次从高到低排序的，即序号是1出现次数最多\n",
    "comment['sent'] = comment['words'].apply(get_sent) #速度太慢，将分的词换成计算频次dataframe中的行序号，默认是按频次从高到低排序的，即序号是1出现次数最多\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "maxlen = 6#输入词序列长度不一样，只截取每个句子切成词序列后的2个词标，计算\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "content_tag['sent'] = list(sequence.pad_sequences(content_tag['sent'], maxlen=maxlen))#每一行的词个数变成50，pad_sequences可以自己检测选择重要（出现次数多）的词？\n",
    "comment['sent'] = list(sequence.pad_sequences(comment['sent'], maxlen=maxlen))#每一行的词个数变成50，pad_sequences可以自己检测选择重要（出现次数多）的词？\n",
    "\n",
    "import numpy as np \n",
    "train_x=np.array(list(content_tag['sent'][1:]))\n",
    "train_y=np.array(list(content_tag[1:].iloc[:,1]))\n",
    "\n",
    "#train_y=np.array(list(content_tag['mark']))\n",
    "test_x=np.array(list(comment['sent']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,  131,   53,    5],\n",
       "       [   0,    0,    0,    0,    0,   35],\n",
       "       [   1,    1,   34,  233, 1323,    6],\n",
       "       ...,\n",
       "       [   0,    0,    0,   31,   51,   32],\n",
       "       [   0,    0,    0,    0,    0,  326],\n",
       "       [ 280,   10,  369,  259,    3,   15]], dtype=int32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_tag['sent'] = content_tag['words'].apply(get_sent) #速度太慢，将分的词换成计算频次dataframe中的行序号，默认是按频次从高到低排序的，即序号是1出现次数最多\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3, 111,  87,   1, 111, 726], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_tag['sent'].iloc[1021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_tag['sent'] = list(sequence.pad_sequences(content_tag['sent'], maxlen=8))#每一行的词个数变成50，pad_sequences可以自己检测选择重要（出现次数多）的词？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3, 111,  87,   1, 111, 726], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_tag['sent'].iloc[1021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuelun/anaconda3/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2402/2402 [==============================] - 45s 19ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2402/2402 [==============================] - 43s 18ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2402/2402 [==============================] - 38s 16ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2402/2402 [==============================] - 36s 15ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2402/2402 [==============================] - 42s 17ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2402/2402 [==============================] - 46s 19ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2402/2402 [==============================] - 52s 21ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "2402/2402 [==============================] - 52s 22ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "2402/2402 [==============================] - 49s 20ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "2402/2402 [==============================] - 49s 21ms/step - loss: nan - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x182db962e8>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=16, nb_epoch=10) #训练时间为若干个小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuelun/anaconda3/lib/python3.6/site-packages/keras/models.py:1181: RuntimeWarning: invalid value encountered in greater\n",
      "  return (proba > 0.5).astype('int32')\n"
     ]
    }
   ],
   "source": [
    "classes = model.predict_classes(train_x)\n",
    "#acc = np_utils.accuracy(classes, yt)\n",
    "#print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-f1cc595d54a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils \n",
    "count=1\n",
    "for i in len(train_y):\n",
    "    if classes[i][0]==train_y[i][0]:\n",
    "        count=count+1 \n",
    "print(count/len(train_y))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=np.array(list(content_tag[1:].iloc[:,1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2402/2402 [==============================] - 1s 376us/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(train_x, train_y)\n",
    "\n",
    "#acc = np_utils.accuracy(classes, train_y)\n",
    "#print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/m2/vy0jdnls50x9z9zr96vrqmj40000gn/T/jieba.cache\n",
      "Loading model cost 0.925 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#neg['mark']=0 #给训练语料贴上标签\n",
    "#pn=pd.concat([pos,neg],ignore_index=True) #合并语料\n",
    "#neglen=len(neg)\n",
    "#poslen=len(pos) #计算语料数目\n",
    "#cw = lambda x: list(jieba.cut(x)) #定义分词函数\n",
    "#pn['words'] = pn[0].apply(cw)#pn[0]是原始从excel读出来的\n",
    "#cw = lambda x: list(jieba.cut(x)) #定义分词函数\n",
    "#pn['words'] = pn[0].apply(cw)#pn[0]是原始从excel读出来的\n",
    "\n",
    "#comment = pd.read_excel('sum.xls') #读入评论内容\n",
    "comment = pd.read_excel('comment.xls') #读入评论内容\n",
    "\n",
    "#comment = pd.read_csv('a.csv', encoding='utf-8')\n",
    "#comment = comment[comment['rateContent'].notnull()] #仅读取非空评论\n",
    "comment['words'] = comment['comment'].apply(cw) #评论分词 \n",
    "\n",
    "d2v_train = pd.concat([pn['words'], comment['words']], ignore_index = True) \n",
    "\n",
    "w = [] #将所有词语整合在一起\n",
    "for i in d2v_train:\n",
    "    w.extend(i)\n",
    "dict = pd.DataFrame(pd.Series(w).value_counts()) #统计词的出现次数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del w,d2v_train\n",
    "dict['id']=list(range(1,len(dict)+1))\n",
    "\n",
    "get_sent = lambda x: list(dict['id'][x])\n",
    "pn['sent'] = pn['words'].apply(get_sent) #速度太慢，将分的词换成计算频次dataframe中的行序号，默认是按频次从高到低排序的，即序号是1出现次数最多\n",
    "comment['sent'] = comment['words'].apply(get_sent) #速度太慢，将分的词换成计算频次dataframe中的行序号，默认是按频次从高到低排序的，即序号是1出现次数最多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "maxlen = 2#输入词序列长度不一样，只截取每个句子切成词序列后的2个词标，计算\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "pn['sent'] = list(sequence.pad_sequences(pn['sent'], maxlen=maxlen))#每一行的词个数变成50，pad_sequences可以自己检测选择重要（出现次数多）的词？\n",
    "comment['sent'] = list(sequence.pad_sequences(comment['sent'], maxlen=maxlen))#每一行的词个数变成50，pad_sequences可以自己检测选择重要（出现次数多）的词？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "train_x=np.array(list(pn['sent']))\n",
    "train_y=np.array(list(pn['mark']))\n",
    "test_x=np.array(list(comment['sent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21105"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "neg=pd.read_excel('neg.xls',header=None,index=None)\n",
    "pos=pd.read_excel('pos.xls',header=None,index=None) #读取训练语料完毕\n",
    "pos['mark']=1\n",
    "neg['mark']=0 #给训练语料贴上标签\n",
    "pn=pd.concat([pos,neg],ignore_index=True) #合并语料\n",
    "neglen=len(neg)\n",
    "poslen=len(pos) #计算语料数目\n",
    "\n",
    "cw = lambda x: list(jieba.cut(x)) #定义分词函数\n",
    "pn['words'] = pn[0].apply(cw)\n",
    "\n",
    "comment = pd.read_excel('sum.xls') #读入评论内容\n",
    "#comment = pd.read_csv('a.csv', encoding='utf-8')\n",
    "comment = comment[comment['rateContent'].notnull()] #仅读取非空评论\n",
    "comment['words'] = comment['rateContent'].apply(cw) #评论分词 \n",
    "\n",
    "d2v_train = pd.concat([pn['words'], comment['words']], ignore_index = True) \n",
    "\n",
    "w = [] #将所有词语整合在一起\n",
    "for i in d2v_train:\n",
    "    w.extend(i)\n",
    "\n",
    "dict = pd.DataFrame(pd.Series(w).value_counts()) #统计词的出现次数\n",
    "del w,d2v_train\n",
    "dict['id']=list(range(1,len(dict)+1))\n",
    "\n",
    "get_sent = lambda x: list(dict['id'][x])\n",
    "pn['sent'] = pn['words'].apply(get_sent) #速度太慢\n",
    "\n",
    "maxlen = 50\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "pn['sent'] = list(sequence.pad_sequences(pn['sent'], maxlen=maxlen))\n",
    "\n",
    "x = np.array(list(pn['sent']))[::2] #训练集\n",
    "y = np.array(list(pn['mark']))[::2]\n",
    "xt = np.array(list(pn['sent']))[1::2] #测试集\n",
    "yt = np.array(list(pn['mark']))[1::2]\n",
    "xa = np.array(list(pn['sent'])) #全集\n",
    "ya = np.array(list(pn['mark']))\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(dict)+1, 256))\n",
    "model.add(LSTM(128)) # try using a GRU instead, for fun\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   23,  5294,   873, ...,     5, 11034,     3],\n",
       "       [  906,   182,    13, ...,   599,  2886,     3],\n",
       "       [  836,     1,    46, ...,    12,   537,     3],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     9,     9,     9],\n",
       "       [   23,   225,  1885, ..., 51951, 48102,     9],\n",
       "       [    0,     0,     0, ...,    15,    19,     3]], dtype=int32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuelun/anaconda3/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10553/10553 [==============================] - 258s 24ms/step - loss: 0.0085 - acc: 0.9976\n",
      "Epoch 2/10\n",
      "10553/10553 [==============================] - 220s 21ms/step - loss: 0.0095 - acc: 0.9972\n",
      "Epoch 3/10\n",
      "10553/10553 [==============================] - 229s 22ms/step - loss: 0.0058 - acc: 0.9984\n",
      "Epoch 4/10\n",
      " 8032/10553 [=====================>........] - ETA: 51s - loss: 0.0012 - acc: 0.9998"
     ]
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=16, nb_epoch=10) #训练时间为若干个小时\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict_classes(train_x)\n",
    "#acc = np_utils.accuracy(classes, yt)\n",
    "#print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44712739383846795\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils \n",
    "count=1\n",
    "\n",
    "for i in range(len(train_y)):\n",
    "    if classes[i][0]==train_y[i]:\n",
    "        count=count+1 \n",
    "print(count/len(train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = model.predict_classes(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
